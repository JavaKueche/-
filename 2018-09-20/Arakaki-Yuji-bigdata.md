## HadoopとHive
Hadoopは「多数のコンピュータで対象のデータ処理を行う」ためのシステムで、Googleで開発された「MapReduce」を参考にして作られた分散処理のフレームワークです。
Hadoopを使ってデータ処理を行うにはJava言語によるプログラミングが必要だったがだれにでも簡単に使えるものではないので、SQLのようなクエリ言語をHadoop上で実行するためのソフトウェアとしてHiveがある。

## NoSQLに書き込み、Hadoopで分散処理
NoSQLは製品によって特徴を様々ですが、RDBよりも高速な読み書きが可能かつ分散処理に優れているという特徴をもっているため、アプリケーションからオンラインで接続するデータベースと利用し、バックエンドでHadoopが分散処理をして集計・データ加工などを行うという形が一般に広がってきた。

## ビッグデータ時代のデータ分析基盤
ビッグデータ自体の技術が従来のデータウェアハウスと違うのは、「多数のぶんさんシステムを組み合わせて拡張性のたかいデータ処理の仕組みをつくる」ことです。

データ分析基盤は以下のようなサブシステムを組み合わせてデータパイプラインを実現していきます。

### データ収集
データを集めるところ。ここからすべてが始まる。
ファイルシステムにあるログファイルから、IoTデバイスのセンサ情報など様々。
データ転送方法には「バルク型」と「ストリーム型」がある

###  ストリーム処理とバッチ処理
データ収集された処理を扱う方法にはストリーム処理とバッチ処理がある。
リアルタイム性が必要なものはストリーム処理。長期的なデータ分析にはバッチ処理が適している。

### 分散ストレージ
集めたデータを補完しておく分散ストレージ。AmazonのS3のようなオブジェクトストレージが一般的に使われる。
データ容量が増えていくので、いくらでも増やせるようなスケーラビリティの高い製品を選ぶ必要あり。

### 分散データ処理
Hadoop・MapReduceなどはこの分野。

### ワークフロー管理
データパイプライン全体の動作管理するための技術。
データパイプラインが複雑化するにつれて一箇所でマネージメントしないと管理できなくなる。


## データレイクとデータウェアハウス
データウェアハウスは従来から大量のデータ分析を行うために使われていた技術。しかしデータウェアハウスは事前にデータ設計をする必要があるがありますが、ビックデータ時代は最初から適したデータ形式を作ることを想定するのが難しい。
そのためデータレイクという考え方があり、それは「生データをとりあえずおいておく」ということ。
オブジェクトストレージなどにおいておいて、あとで適した形式にHadoopなどを使ってデータマートを作るという考え方。
